{
    "docs": [
        {
            "location": "/", 
            "text": "Niagara Data Platform \n\n\nNiagara is a Fast \n Big Data Processing, Machine Learning, and Data-as-a-Service platform, implemented in Scala with SDACK stack.\nIt is built on complicated public data sets to evaluate emerging Stateful Stream Processing to build lightweight Streaming Services.\n\n\nSDACK Tech Stack\n\n\n\n\n\n\nThe batch analytic engine: Spark (Spark Streaming, SQL, MLlib)\n\n\n\n\n\n\nThe lightweight container: Docker (Kubernetes)\n\n\n\n\n\n\nThe real-time view: Akka (Akka Streams, Http, Alpakka)\n\n\n\n\n\n\nThe scalable storage: Cassandra\n\n\n\n\n\n\nThe distributed message broker: Kafka (Kafka Streams, Connects, Schema Registry)\n\n\n\n\n\n\nDataset\n\n\n\n\n\n\nThe Yelp Dataset contains 4.1 million reviews(3.5GB) by 1 million users(1.2GB) for 144K businesses(115MB).\nhttps://www.yelp.ca/dataset_challenge\n\n\n\n\n\n\nThe Stack Exchange Dataset contains 28 million Posts in a 40GB single XML file.\nhttps://archive.org/details/stackexchange\n\n\n\n\n\n\nModules\n\n\n\n\n\n\nData Streaming (Kafka, Spark, Akka)\n\n\n\n\n\n\nCQRS \n Event Sourcing\n\n\n\n\n\n\nMachine Learning", 
            "title": "Home"
        }, 
        {
            "location": "/#niagara-data-platform", 
            "text": "Niagara is a Fast   Big Data Processing, Machine Learning, and Data-as-a-Service platform, implemented in Scala with SDACK stack.\nIt is built on complicated public data sets to evaluate emerging Stateful Stream Processing to build lightweight Streaming Services.", 
            "title": "Niagara Data Platform "
        }, 
        {
            "location": "/#sdack-tech-stack", 
            "text": "The batch analytic engine: Spark (Spark Streaming, SQL, MLlib)    The lightweight container: Docker (Kubernetes)    The real-time view: Akka (Akka Streams, Http, Alpakka)    The scalable storage: Cassandra    The distributed message broker: Kafka (Kafka Streams, Connects, Schema Registry)", 
            "title": "SDACK Tech Stack"
        }, 
        {
            "location": "/#dataset", 
            "text": "The Yelp Dataset contains 4.1 million reviews(3.5GB) by 1 million users(1.2GB) for 144K businesses(115MB).\nhttps://www.yelp.ca/dataset_challenge    The Stack Exchange Dataset contains 28 million Posts in a 40GB single XML file.\nhttps://archive.org/details/stackexchange", 
            "title": "Dataset"
        }, 
        {
            "location": "/#modules", 
            "text": "Data Streaming (Kafka, Spark, Akka)    CQRS   Event Sourcing    Machine Learning", 
            "title": "Modules"
        }, 
        {
            "location": "/install/", 
            "text": "Installation\n\n\nInstall Java 8 and Scala 2.11 in Ubuntu\n\n\nPlease check the steps in my \nBig Data Blog\n\n\nCreate Kafka and Zookeeper Docker Containers\n\n\nInstall Docker for Mac OS, then create 3 containers for Zookeeper, Kafka broker and Schema Registry, respectively.\n\n\n$ cd ~/pathto/Niagara\n\n//Start containers\n$ docker-compose up\n\n\n//Stop containers and remove them entirely\n$ docker-compose down\n\n\n\n\n\nInstall Cassandra\n\n\nDownload and unzip \nCassandra 3.1.0+\n\n\n$ cd apache-cassandra-3.1.0\n$ ./bin/cassandra\n\n\n\n\nBuild and Run the App\n\n\nUse \nsbt-avrohugger\n to automatically generate SpecificRecord case class from avro schema.\n\n\n$ sbt avro:generate-specific\n\n\n\n\nBuild an uber jar with all the dependencies.\n\n\n$ sbt clean assembly", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/install/#install-java-8-and-scala-211-in-ubuntu", 
            "text": "Please check the steps in my  Big Data Blog", 
            "title": "Install Java 8 and Scala 2.11 in Ubuntu"
        }, 
        {
            "location": "/install/#create-kafka-and-zookeeper-docker-containers", 
            "text": "Install Docker for Mac OS, then create 3 containers for Zookeeper, Kafka broker and Schema Registry, respectively.  $ cd ~/pathto/Niagara\n\n//Start containers\n$ docker-compose up\n\n\n//Stop containers and remove them entirely\n$ docker-compose down", 
            "title": "Create Kafka and Zookeeper Docker Containers"
        }, 
        {
            "location": "/install/#install-cassandra", 
            "text": "Download and unzip  Cassandra 3.1.0+  $ cd apache-cassandra-3.1.0\n$ ./bin/cassandra", 
            "title": "Install Cassandra"
        }, 
        {
            "location": "/install/#build-and-run-the-app", 
            "text": "Use  sbt-avrohugger  to automatically generate SpecificRecord case class from avro schema.  $ sbt avro:generate-specific  Build an uber jar with all the dependencies.  $ sbt clean assembly", 
            "title": "Build and Run the App"
        }, 
        {
            "location": "/stream/", 
            "text": "Streaming Frameworks\n\n\nKafka Streams\n\n\nIn many use cases, we have to support an event-by-event low latency model rather than one that focuses on microbatches,\nand deliver upstream changes to the materialized state store to serve microservice.\n\n\nA Spark core app ingests Json files and converts Json to Avro messages in Kafka topics, e.g. review, business, user.\n\n\nA Kafka streams application consumes review messages as KStream from review topic.\nIn the meanwhile, it consumes Business and User data as GlobalKTable.\n\n\nKStream works like Fact table, containing large volume immutable transactional records.\nWhile KTable works like Dimension table, contains small volume domain data snapshot.\nThe pipeline enriches Review Stream by joining with Business and User KTables in real-time to a new Kafka topic.\nThen, apply filtering, aggregations and keep the results in local state store to provide Micro-service for Interactive Queries.\n\n\nFor example, KeyValue query on stars summed by city\n\n\nGET /stars/{city}\n\n\n\n\nRange Query on stars sumed by business in a time window\n\n\nGET /stars/{business}/{from}/{to}\n\n\n\n\nAkka Streams\n\n\nThe Akka Streams application ingests XML format posts by the file connector in Alpakka.\nAkka streams producer app forms a real-time data pipeline to parse, enrich and transform XML files to Avro messages in a Kafka topic.\n\n\nAn Akka stream consumer app consumes avro messages from Kafka, and then persists data into Cassandra database.\nThe data flow DSL is shown below:\n\n\nxmlSource ~\n parsing ~\n broadcast ~\n enrich1 ~\n merge ~\n fillTitle ~\n kafkaSink\n                        broadcast ~\n enrich2 ~\n merge\n\n\n\n\nThe service layer provides RESTful APIs built by Akka-Http for users to easily interact with data for ad-hoc analytics.\nUnder the hood, the service calls Cassandra APIs to implement CRUD operations.\n\n\nSpark Streaming\n\n\nA Spark core app ingests Json files and converts Json to Avro messages in Kafka topics, e.g. review, business, user.\nA Spark streaming consumer consumes data from Kafka, and then do the transformation and aggregation.\nSelect features and persist them into Cassandra.", 
            "title": "Data Streaming"
        }, 
        {
            "location": "/stream/#streaming-frameworks", 
            "text": "", 
            "title": "Streaming Frameworks"
        }, 
        {
            "location": "/stream/#kafka-streams", 
            "text": "In many use cases, we have to support an event-by-event low latency model rather than one that focuses on microbatches,\nand deliver upstream changes to the materialized state store to serve microservice.  A Spark core app ingests Json files and converts Json to Avro messages in Kafka topics, e.g. review, business, user.  A Kafka streams application consumes review messages as KStream from review topic.\nIn the meanwhile, it consumes Business and User data as GlobalKTable.  KStream works like Fact table, containing large volume immutable transactional records.\nWhile KTable works like Dimension table, contains small volume domain data snapshot.\nThe pipeline enriches Review Stream by joining with Business and User KTables in real-time to a new Kafka topic.\nThen, apply filtering, aggregations and keep the results in local state store to provide Micro-service for Interactive Queries.  For example, KeyValue query on stars summed by city  GET /stars/{city}  Range Query on stars sumed by business in a time window  GET /stars/{business}/{from}/{to}", 
            "title": "Kafka Streams"
        }, 
        {
            "location": "/stream/#akka-streams", 
            "text": "The Akka Streams application ingests XML format posts by the file connector in Alpakka.\nAkka streams producer app forms a real-time data pipeline to parse, enrich and transform XML files to Avro messages in a Kafka topic.  An Akka stream consumer app consumes avro messages from Kafka, and then persists data into Cassandra database.\nThe data flow DSL is shown below:  xmlSource ~  parsing ~  broadcast ~  enrich1 ~  merge ~  fillTitle ~  kafkaSink\n                        broadcast ~  enrich2 ~  merge  The service layer provides RESTful APIs built by Akka-Http for users to easily interact with data for ad-hoc analytics.\nUnder the hood, the service calls Cassandra APIs to implement CRUD operations.", 
            "title": "Akka Streams"
        }, 
        {
            "location": "/stream/#spark-streaming", 
            "text": "A Spark core app ingests Json files and converts Json to Avro messages in Kafka topics, e.g. review, business, user.\nA Spark streaming consumer consumes data from Kafka, and then do the transformation and aggregation.\nSelect features and persist them into Cassandra.", 
            "title": "Spark Streaming"
        }, 
        {
            "location": "/escqrs/", 
            "text": "CQRS \n Event Sourcing\n\n\nWith the distributed guarantees of Exactly Once Processing, Event Driven Services supported by Apache Kafka become reliable, fast and nimble,\nblurring the line between transactional business system and big data pipeline.\nPlease check my \nBlog Post\n for the details of CQRS \n Event Sourcing.\nCDC(Chang Data Capture) is an approach to stream the database changes from binlogs to Kafka State Store.", 
            "title": "Event Sourcing & CQRS"
        }, 
        {
            "location": "/escqrs/#cqrs-event-sourcing", 
            "text": "With the distributed guarantees of Exactly Once Processing, Event Driven Services supported by Apache Kafka become reliable, fast and nimble,\nblurring the line between transactional business system and big data pipeline.\nPlease check my  Blog Post  for the details of CQRS   Event Sourcing.\nCDC(Chang Data Capture) is an approach to stream the database changes from binlogs to Kafka State Store.", 
            "title": "CQRS &amp; Event Sourcing"
        }, 
        {
            "location": "/ml/", 
            "text": "Machine Learning\n\n\nDeep Learning\n\n\nComing soon ...", 
            "title": "Machine Learning"
        }, 
        {
            "location": "/ml/#machine-learning", 
            "text": "", 
            "title": "Machine Learning"
        }, 
        {
            "location": "/ml/#deep-learning", 
            "text": "Coming soon ...", 
            "title": "Deep Learning"
        }, 
        {
            "location": "/about/", 
            "text": "About\n\n\nAlvin Jin is Sr. Big Data \n Machine Learning Tech Lead @Toronto\n:fireworks:\nEmail: alvincjin@gmail.com", 
            "title": "About"
        }, 
        {
            "location": "/about/#about", 
            "text": "Alvin Jin is Sr. Big Data   Machine Learning Tech Lead @Toronto\n:fireworks:\nEmail: alvincjin@gmail.com", 
            "title": "About"
        }
    ]
}